{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Change Detection on Streaming Data\n",
    "\n",
    "https://github.com/codyrioux/pydata2015seattle/\n",
    "\n",
    "In this tutorial we will be tackling the task of online change detection in data streams, which is perhaps unsurprisingly detecting \"change\" in streams of data in real-time. The objective of this tutorial is to get you from zero to being capable of detecting change and potentially implementing your own solutions. We will use the following formal definition for our problem:\n",
    "\n",
    "> If we have an ordered incoming stream of observations at discrete time intervals, then we can imagine there is some underlying generating mechanism producing the observations we are receiving in this stream. The problem of detecting change is then simply inferring from the observation stream if there has been a change to the underlying generating mechanism.\n",
    "\n",
    "For practical purposes in this tutorial we will use streams of doubles either generated by a synthetic data generator or a previously captured data stream at Netflix, however the objective is to explore online techniques which operate on data streams which we have not yet completely captured. To this end we will be generating synthetic data streams and processing previously captured data in a streaming fashion.\n",
    "\n",
    "A great example of detecting change in an offline fashion would in the first chapter of Cam's book [Bayesian Methods for Hackers](http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-oand-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Chapter1.ipynb) in which he takes approximately two months of daily text message counts and attempts to determine if his texting habits changed over the course of the month. I've shamelessly lifted the image, and his data looks like this:\n",
    "\n",
    "<img src=\"texting.png\">\n",
    "\n",
    "The analysis performed identifies likely distributions for texting habits before and after the change, as well as the most probable days for the change. We would likel to do this in an online fashion, and we'll be satisfied with detecting the point of change as accurately as possible. Reimagine this problem except processing the days one at a time each day and triggering an alert when we suspect the stream's generator has changed. Lets observe a handful of examples which display different change behaviors that we would like to detect: \n",
    "\n",
    "<img src=\"change1.png\">\n",
    "<img src=\"change2.png\">\n",
    "<img src=\"change3.png\">\n",
    "\n",
    "So where might you find something like this useful? The answer is any metric on which it would be useful to be alerted of change might be effective. This may be a metric as simple as something that should stay relatively constant such as the temperature in a server room (a threshold may be more appropriate here but you get the idea) or something that drifts over time such as signups per hour for a small startup. At Netflix change detection is part of a larger analytics suite and is often used in conjuction with other analytics but one excellent example of where its used is the call volume for a specific device in our call centre. If the generating mechanism for this has changed it is a strong indicator that the device may be experiencing an issue streaming Netflix.\n",
    "\n",
    "<img src=\"calls.png\">\n",
    "\n",
    "## Challenges\n",
    "\n",
    "There are a number of challenges when dealing with detecting change in real-time, I'll outline a few of them here so you can think about them while we go through this tutorial.\n",
    "\n",
    "- False Positives: Detecting change when there is none, an algorithm that is too sensitive. This is especially an issue in production when mathematical change doesn't necessarily imply change that a human cares about.\n",
    "- False Negatives: Failing to detect change when there is some change, an algorithm that is too insensitive.\n",
    "- Detection Lag: How quickly are we able to detect change?\n",
    "\n",
    "Different techniques will address these challenges differently, some will be weak in some areas and strong in others. Furthermore it depends on the characteristics of our data streams. Later when implementing your own solutions consider how they'll interact with the points above.\n",
    "\n",
    "\n",
    "## Components\n",
    "We require a handful of components to solve this problem effectively. These components include a simulator, change detector base classes, data generators and evaluation functions. Together these components can be used to construct and evaluate experiments against one another. A number of components are included in this package, though they've been kept intentionally simple for understandability. They should however also form an excellent base for a more robust system should you chose to implement any of these components directly. We'll require the following components:\n",
    "\n",
    "\n",
    "- Detector Framework\n",
    "- Data Generators\n",
    "- Simulation Framework\n",
    "- Evaluation Functions\n",
    "- Basic Change Detector (Normal Approximation)\n",
    "- Monte Carlo Change Detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Schedule\n",
    "\n",
    "This tutorial is slated to be 110 minutes long, I can't imagine talking for 110 minutes. I've got just over an hour of material, the rest will be Q&A or an early release.\n",
    "\n",
    "- Introduction: 10 Minutes\n",
    "- Generators: 15 Minutes\n",
    "- Evaluation Functions: 5 Minutes\n",
    "- Framework (Simulator + Detectors): 10 Minutes\n",
    "- Simple Detector: 10 Minutes\n",
    "- Monte Carlo Detector: 10 Minutes\n",
    "- Case Study: 10 Minutes\n",
    "- Q&A: 30 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data\n",
    "A typical concern when working on streaming algorithms is data collection. If you've ever attempted to build or acquire a large well curated data set you know that its expensive on either time or money depending on how you go about it. Streaming data is even more of a problem because unless you capture the data it is lost, and sometimes the data is streamed because its too expensive to store.\n",
    "\n",
    "Possible solutions to this are to observe your data stream and capture the necessary data, but you still need to manually process it and tag the data. We've got some captured data from Netflix in this tutorial, but its not enough... Another solution commonly applied to this problem is synthetic data generation. We'll create a class of data generators which create fake datasets for us with known changepoints for the purposes of testing our algorithms against a large number of data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._changepoint = -1\n",
    "    \n",
    "    def get(self):\n",
    "        self._changepoint += 1\n",
    "        return 1.0\n",
    "    \n",
    "class DistributionGenerator(Generator):\n",
    "    \"\"\"\n",
    "    A generator which generates values from a single distribution. This may not immediately\n",
    "    appear useful for change detection, however if we can model our unchanged data stream\n",
    "    as a distribution then we can test against false positives by running tests against\n",
    "    a single distribution.\n",
    "    \n",
    "    dist1:  A scipy.stats distribution for before changepoint.\n",
    "    kwargs: The keyword arguments are passed to the distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dist, **kwargs):\n",
    "        self._dist = dist\n",
    "        self._args = kwargs\n",
    "        \n",
    "    def get(self):\n",
    "        return self._dist.rvs(**self._args)\n",
    "\n",
    "    \n",
    "class ChangingDistributionGenerator(Generator):\n",
    "    \"\"\"\n",
    "    A generator which takes two distinct distributions and a changepoint and returns\n",
    "    random variates from the first distribution until it has reached the changepoint\n",
    "    when it then switches to the next.\n",
    "    \n",
    "    dist1:       A scipy.stats distribution for before changepoint.\n",
    "    kwargs1:     A map specifying loc and scale for dist1.\n",
    "    dist2:       A scipy.stats distribution for after changepoint.\n",
    "    kwargs2:     A map specifying loc and scale for dist2.\n",
    "    changepoint: The number of values to be generated before switching to dist2.\n",
    "    \"\"\"\n",
    "    \n",
    "    _position = 0\n",
    "    \n",
    "    def __init__(self, dist1, kwargs1, dist2, kwargs2, changepoint):\n",
    "        self._dist1 = dist1\n",
    "        self._kwargs1 = kwargs1\n",
    "        self._dist2 = dist2\n",
    "        self._kwargs2 = kwargs2\n",
    "        self._changepoint = changepoint\n",
    "        \n",
    "    def get(self):\n",
    "        self._position += 1\n",
    "        if self._position <= self._changepoint:\n",
    "            return self._dist1.rvs(**self._kwargs1)\n",
    "        else:\n",
    "            return self._dist2.rvs(**self._kwargs2)\n",
    "\n",
    "        \n",
    "class DriftGenerator(Generator):\n",
    "    \"\"\"\n",
    "    A generator which takes two distinct distributions and a changepoint and returns\n",
    "    random variates from the first distribution until it has reached the changepoint\n",
    "    when it then drifts to the next.\n",
    "    \n",
    "    dist1:       A scipy.stats distribution for before changepoint.\n",
    "    kwargs1:     A map specifying loc and scale for dist1.\n",
    "    dist2:       A scipy.stats distribution for after changepoint.\n",
    "    kwargs2:     A map specifying loc and scale for dist2.\n",
    "    changepoint: The number of values to be generated before switching to dist2.\n",
    "    steps:       The number of time steps to spend drifting to dist2.\n",
    "    \"\"\"\n",
    "    \n",
    "    _position = 0\n",
    "    \n",
    "    def __init__(self, dist1, kwargs1, dist2, kwargs2, changepoint, steps):\n",
    "        self._dist1 = dist1\n",
    "        self._kwargs1 = kwargs1\n",
    "        self._dist2 = dist2\n",
    "        self._kwargs2 = kwargs2\n",
    "        self._changepoint = changepoint\n",
    "        self._steps = steps\n",
    "        \n",
    "        self._change_gradient = np.linspace(0, 1, self._steps)\n",
    "        \n",
    "    def get(self):\n",
    "        self._position += 1\n",
    "        if self._position < self._changepoint:\n",
    "            return self._dist1.rvs(**self._kwargs1)\n",
    "        if self._position >= self._changepoint and self._position < self._changepoint + self._steps:\n",
    "            beta = self._change_gradient[self._position - self._changepoint - 1]\n",
    "            return ((1 - beta) * self._dist1.rvs(**self._kwargs1)) + (beta * self._dist2.rvs(**self._kwargs2))\n",
    "        else:\n",
    "            return self._dist2.rvs(**self._kwargs2)\n",
    "        \n",
    "class DataBackedGenerator(Generator):\n",
    "    \"\"\"\n",
    "    A generator which takes a vector of values and behaves similarly\n",
    "    to the other generators here. Returns None if values are requested\n",
    "    past the end of the supplied vector.\n",
    "    \n",
    "    vec:         The vector of values for this generator to produce.\n",
    "    changepoint: The index at which the change occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    _idx = 0\n",
    "    \n",
    "    def __init__(self, vec, changepoint):\n",
    "        self._vec = vec\n",
    "        self._changepoint = changepoint\n",
    "        \n",
    "    def get(self):\n",
    "        if self._idx < len(self._vec):\n",
    "            self._idx += 1\n",
    "            return self._vec[self._idx - 1]\n",
    "        \n",
    "        \n",
    "# TODO: Exercise with group\n",
    "class DriftingDistributionGenerator(Generator):\n",
    "    \"\"\"\n",
    "    A generator which takes two distinct distributions and a changepoint and returns\n",
    "    random variates from the first distribution until it has reached the changepoint\n",
    "    where it linearly drifts to the second distribution.\n",
    "    \n",
    "    dist1:       A scipy.stats distribution for before changepoint.\n",
    "    kwargs1:     A map specifying loc and scale for dist1.\n",
    "    dist2:       A scipy.stats distribution for after changepoint.\n",
    "    kwargs2:     A map specifying loc and scale for dist2.\n",
    "    changepoint: The number of values to be generated before switching to dist2.\n",
    "    steps:       The number of steps to take in the drift.\n",
    "    \"\"\"\n",
    "    \n",
    "    _position = 0\n",
    "    \n",
    "    def __init__(self, dist1, kwargs1, dist2, kwargs2, changepoint, steps):\n",
    "        self._dist1 = dist1\n",
    "        self._kwargs1 = kwargs1\n",
    "        self._dist2 = dist2\n",
    "        self._kwargs2 = kwargs2\n",
    "        self._changepoint = changepoint\n",
    "        self._steps = steps\n",
    "        \n",
    "        self._change_gradient = np.linspace(0, 1, self._steps)\n",
    "        \n",
    "    def get(self):\n",
    "        self._position += 1\n",
    "        if self._position <= self._changepoint:\n",
    "            return self._dist1.rvs(**self._kwargs1)\n",
    "        elif self._position > self._changepoint and self._position <= self._changepoint + self._steps:\n",
    "            beta = self._change_gradient[self._position - self._changepoint - 1]\n",
    "            return ((1.0 - beta) * self._dist1.rvs(**self._kwargs1)) + (beta * self._dist2.rvs(**self._kwargs2))\n",
    "        else:\n",
    "            return self._dist2.rvs(**self._kwargs2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Examples\n",
    "Lets Play... generate, plot, etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "We require some method of quantifying the performance of our techniques, which will allow us to objectively compare two approaches in a straightforward fashion. I personally like loss functions, and for an excellent hacker's introduction to them refer to Chapter 5 of [Bayesian Methods for Hackers](http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter5_LossFunctions/Chapter5.ipynb). If you refer back to our challenges in the introduction this is an excellent opportunity to express preference for some, for example you might design a loss function that penalizes false positives more heavily than false negatives because you're using the algorithm to alert users and you'd rather it didn't wake anyone up in the middle of the night accidentally.\n",
    "\n",
    "We'll keep it simple in this tutorial and rely on squared error loss. We'll be squaring the difference between the actual changepoint and the detected one meaning we're expressing no preference for detecting early or late (this may be a detail we don't want to ignore). Furthermore for batches of evaluations we'll use root mean squared error, effectively measuring the average time to detect in which time is the number of time steps in our data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r_squared_error_loss(target_point, stop_point):\n",
    "    \"\"\"\n",
    "    Returns the root squared error loss when given the target point and stop point.\n",
    "    target_point: The known point at which the signal changed.\n",
    "    stop_point: The point at which the algorithm deteremined a stop should be performed.\n",
    "    \n",
    "    Returns: Root squared error loss between the two values.\n",
    "    \"\"\"\n",
    "    return math.sqrt((target_point - stop_point) ** 2)\n",
    "\n",
    "def root_mean_squared_error_loss(target_points, stop_points):\n",
    "    \"\"\"\n",
    "    Returns the root mean squared error (RMSE) loss for a series of target values,\n",
    "    and actual selected values.\n",
    "    target_points: The known points at which the signal changed.\n",
    "    stop_points: The points at which the algorithm deteremined a stop should be performed.\n",
    "    \n",
    "    Returns: Root mean squared error between the two sets.\n",
    "    \"\"\"\n",
    "    cumulative_loss = 0.0\n",
    "    for i in xrange(len(target_points)):\n",
    "        cumulative_loss += (target_points[i] - stop_points[i]) ** 2\n",
    "    return math.sqrt(cumulative_loss / (1.0 * len(target_points)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets play again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectors\n",
    "Detectors are the workhorse of this tutorial, they can be very simple or incredibly complex. There are many different techniques which we can utilize to detect change. Exploring this topic together, and then self exploration will be the primary topic of this tutorial.\n",
    "\n",
    "First our base change detector will perform an incredibly naive analysis in checking if the incoming value is different than the previous value. This functions properly only in the degenerate case where a stream must stay completely constant. You'll find it doesn't address our previously discussed challenges well.\n",
    "\n",
    "Our second detector will be more intelligent, but still perhaps a little naive. We will implement a windowed threshold based detector which compares the mean of a small window against summary statistics for the entire stream. Drawbacks of this approach should be apparent after running simulations.\n",
    "\n",
    "Finally we will implement and simulate a Monte Carlo sampling approach to online change detection which will be the most robust of the three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ChangeDetector(object):\n",
    "    \"\"\"\n",
    "    The basic interface for our change detectors, it will be the responsibility\n",
    "    of the simulator to check if the detector is triggered.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.triggered = False\n",
    "        self.changepoint = 0\n",
    "        self.count = 0\n",
    "        self._previous = None\n",
    "        \n",
    "    def step(self, datum):\n",
    "        \"\"\"\n",
    "        Performs all the necessary step action for a given detector,\n",
    "        and incredibly naive approach to detecting change.\n",
    "        \n",
    "        Returns: True if change has been detected, False otherwise.\n",
    "        \"\"\"\n",
    "        if self._previous is not None:\n",
    "            if self._previous != datum:\n",
    "                self.triggered = True\n",
    "        self._previous = datum\n",
    "        return self.triggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Simulator(object):\n",
    "    \"\"\"\n",
    "    A basic simulator which takes a set of generator objects\n",
    "    and a detector, running the detector against each generator\n",
    "    once and recording the results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generators, detector, limit=1200):\n",
    "        self._generators = generators\n",
    "        self._detector = detector\n",
    "        self._changepoints = []\n",
    "        self._detected_changepoints = []\n",
    "        self._limit = limit\n",
    "        \n",
    "        for generator in self._generators:\n",
    "            self._changepoints.append(generator._changepoint)\n",
    "            \n",
    "    def plot(self, vals, changepoint, detected_changepoint, title):\n",
    "        plt.plot(vals)\n",
    "        plt.vlines(changepoint, max(vals) * 1.2, 1, color='black')\n",
    "        plt.vlines(detected_changepoint, max(vals), 1, color='red')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        \n",
    "    def run(self, plot=False):\n",
    "        for generator in self._generators:\n",
    "            detector = copy.deepcopy(self._detector)\n",
    "            vals = []\n",
    "            \n",
    "            val = generator.get()\n",
    "            changed = detector.step(val)\n",
    "            vals.append(val)\n",
    "            \n",
    "            while not changed and len(vals) < self._limit:\n",
    "                val = generator.get()\n",
    "                vals.append(val)\n",
    "                changed = detector.step(val)\n",
    "            \n",
    "            if changed:\n",
    "                self._detected_changepoints.append(detector.changepoint)\n",
    "            else:\n",
    "                self._detected_changepoints.append(self._limit)\n",
    "            \n",
    "            if plot:\n",
    "                self.plot(vals, generator._changepoint, detector.changepoint, generator.__class__.__name__)\n",
    "        return root_mean_squared_error_loss(self._changepoints, self._detected_changepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6ce1793b0333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataBackedGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchangepoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindowedMonteCarloDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0msimulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vec' is not defined"
     ]
    }
   ],
   "source": [
    "# Lets run a few simulations with Generator and DistributionGenerator using ChangeDetector\n",
    "\n",
    "class WindowedMonteCarloDetector(ChangeDetector):\n",
    "    \n",
    "    def __init__(self, len1, len2, samples=1000, confidence=0.95, min_samples=50):\n",
    "        self._window1 = deque(maxlen=len1)\n",
    "        self._window2 = deque(maxlen=len2)\n",
    "        self._confidence = confidence\n",
    "        self._min_samples = min_samples\n",
    "        self._samples = samples\n",
    "        \n",
    "        self._N = 0\n",
    "        self._triggered = False\n",
    "        self.changepoint = 0\n",
    "        \n",
    "        \n",
    "    def step(self, datum):\n",
    "        self._window1.append(datum)\n",
    "        self._window2.append(datum)\n",
    "        self._N += 1\n",
    "        \n",
    "        if self._N >= self._min_samples:\n",
    "            diffs = np.zeros(self._samples)\n",
    "            for i in xrange(self._samples):\n",
    "                diffs[i] = random.choice(self._window1) - random.choice(self._window2)\n",
    "            \n",
    "            hdi_min, hdi_max = hdi(diffs, self._confidence)\n",
    "            self._triggered = not between(0.0, hdi_min, hdi_max)\n",
    "            if self._triggered:\n",
    "                self.changepoint = self._N\n",
    "        \n",
    "        return self._triggered\n",
    "\n",
    "\n",
    "generator = DataBackedGenerator(vec, changepoint)\n",
    "detector = WindowedMonteCarloDetector(100, 10, 1000, 0.25)\n",
    "simulator = Simulator([generator], detector)\n",
    "simulator.run(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets implement a simple detector...\n",
    "\n",
    "class ThreshDetector(object):\n",
    "    \n",
    "    def __init__(self, threshold=0.2, window_length=10, min_training=50):\n",
    "        self._window = deque(maxlen=window_length)\n",
    "        self._threshold = threshold\n",
    "        self._triggered = False\n",
    "        self.changepoint = 0\n",
    "        self._min_training = min_training\n",
    "        \n",
    "        self._sum = 0\n",
    "        self._sumsq = 0\n",
    "        self._N = 0\n",
    "        \n",
    "    def step(self, datum):\n",
    "        \n",
    "        self._window.append(datum)\n",
    "        \n",
    "        # Welford's method\n",
    "        self._N += 1\n",
    "        self._sum += datum\n",
    "        self._sumsq += datum ** 2\n",
    "        \n",
    "        self._mu = self._sum / self._N\n",
    "\n",
    "        if self._N > self._min_training:\n",
    "            variance = (self._sumsq - self._N * self._mu ** 2) / (self._N - 1)\n",
    "            self._std = math.sqrt(variance)\n",
    "            \n",
    "            window_mu = sum(self._window) / len(self._window)\n",
    "            ratio = window_mu / self._mu # TODO: Will fail if mu is zero.\n",
    "            if ratio > (1.0 + self._threshold) or ratio < (1.0 - self._threshold):\n",
    "                self._triggered = True\n",
    "                self.changepoint = self._N\n",
    "        return self._triggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets implement a monte carlo detector...     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Netflix Call Volume Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('calls.csv')\n",
    "vec = data['calls'].values\n",
    "vec = np.nan_to_num(vec)\n",
    "changepoint = data['event'].values.nonzero()[0][0]\n",
    "\n",
    "plt.plot(vec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this sort of thing interests you, or you think you could teach me a thing or two I am looking for more [Real-Time Analytics Engineers](https://jobs.netflix.com/jobs/2259/apply) for the Real-Time Analytics team at Netflix. Apply online, but definitely personally reach out to me at the conference or crioux AT netflix DOT com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Research\n",
    "\n",
    "There is a fanstatic ipython notebook on [Bayesian Change Detection](https://github.com/hildensia/bayesian_changepoint_detection) and replicating its work here would be foolish. Assuming the requisite knowledge in my tutorial would also be foolish. As a result I'm linking it here for you, as well as its references.\n",
    "\n",
    "\n",
    "[1] Paul Fearnhead, Exact and Efficient Bayesian Inference for Multiple                                    \n",
    "    Changepoint problems, Statistics and computing 16.2 (2006), pp. 203--213                               \n",
    "                                                                                                           \n",
    "[2] Ryan P. Adams, David J.C. MacKay, Bayesian Online Changepoint Detection,                               \n",
    "    arXiv 0710.3742 (2007)                                                                                 \n",
    "                                                                                                           \n",
    "[3] Xuan Xiang, Kevin Murphy, Modeling Changing Dependency Structure in                                    \n",
    "    Multivariate Time Series, ICML (2007), pp. 1055--1062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def calc_min_interval(x, alpha):\n",
    "    \"\"\"Internal method to determine the minimum interval of\n",
    "    a given width\"\"\"\n",
    "\n",
    "    # Initialize interval\n",
    "    min_int = [None,None]\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Number of elements in trace\n",
    "        n = len(x)\n",
    "\n",
    "        # Start at far left\n",
    "        start, end = 0, int(n*(1-alpha))\n",
    "\n",
    "        # Initialize minimum width to large value\n",
    "        min_width = np.inf\n",
    "\n",
    "        while end < n:\n",
    "\n",
    "            # Endpoints of interval\n",
    "            hi, lo = x[end], x[start]\n",
    "\n",
    "            # Width of interval\n",
    "            width = hi - lo\n",
    "\n",
    "            # Check to see if width is narrower than minimum\n",
    "            if width < min_width:\n",
    "                min_width = width\n",
    "                min_int = [lo, hi]\n",
    "\n",
    "            # Increment endpoints\n",
    "            start +=1\n",
    "            end += 1\n",
    "\n",
    "        return min_int\n",
    "\n",
    "    except IndexError:\n",
    "        print 'Too few elements for interval calculation'\n",
    "        return [None,None]\n",
    "\n",
    "\n",
    "def hdi(trace, cred_mass=0.95):\n",
    "    hdi_min, hdi_max = calc_min_interval(np.sort(trace), 1.0-cred_mass)\n",
    "    return hdi_min, hdi_max\n",
    "\n",
    "def between(val, minval, maxval):\n",
    "    return val >= minval and val <= maxval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
